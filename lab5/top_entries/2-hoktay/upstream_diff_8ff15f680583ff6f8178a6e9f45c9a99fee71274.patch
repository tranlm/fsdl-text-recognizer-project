diff --git a/lab1/text_recognizer/models/character_model.py b/lab1/text_recognizer/models/character_model.py
index fbd0afd..af5219b 100644
--- a/lab1/text_recognizer/models/character_model.py
+++ b/lab1/text_recognizer/models/character_model.py
@@ -21,7 +21,10 @@ class CharacterModel(Model):
             image = (image / 255).astype(np.float32)
         # NOTE: integer to character mapping dictionary is self.data.mapping[integer]
         ##### Your code below (Lab 1)
-
+        pred_raw = self.network.predict(np.expand_dims(image, 0), batch_size=1).flatten()
+        ind = np.argmax(pred_raw)
+        confidence_of_prediction = pred_raw[ind]
+        predicted_character = self.data.mapping[ind]
         ##### Your code above (Lab 1)
         return predicted_character, confidence_of_prediction
 
diff --git a/lab1/text_recognizer/networks/mlp.py b/lab1/text_recognizer/networks/mlp.py
index 7dac32f..5060839 100644
--- a/lab1/text_recognizer/networks/mlp.py
+++ b/lab1/text_recognizer/networks/mlp.py
@@ -1,12 +1,13 @@
 from typing import Tuple
 
+import tensorflow as tf
 from tensorflow.keras.models import Model, Sequential
-from tensorflow.keras.layers import Dense, Dropout, Flatten
+from tensorflow.keras.layers import Dense, Dropout, Flatten, Lambda,MaxPooling2D, Conv2D
 
 
 def mlp(input_shape: Tuple[int, ...],
         output_shape: Tuple[int, ...],
-        layer_size: int=128,
+        layer_size: int=100,
         dropout_amount: float=0.2,
         num_layers: int=3) -> Model:
     """
@@ -16,9 +17,44 @@ def mlp(input_shape: Tuple[int, ...],
     num_classes = output_shape[0]
 
     model = Sequential()
+    if len(input_shape) < 3:
+        model.add(Lambda(lambda x: tf.expand_dims(x, -1), input_shape=input_shape))
+        input_shape = (input_shape[0], input_shape[1], 1)
+
+    model.add(Conv2D(32, 
+                     kernel_size = (3,3), 
+                     strides = (1,1),
+                    activation = 'relu',
+                    input_shape = input_shape))
+    model.add(MaxPooling2D(pool_size = (2,2),
+                          strides = (2,2)))
+    
+    model.add(Conv2D(64, (5, 5), activation='relu'))
+
+    model.add(MaxPooling2D(pool_size=(2, 2)))
+
+    model.add(Flatten())
+
+
     # Don't forget to pass input_shape to the first layer of the model
     ##### Your code below (Lab 1)
 
+#   model.add(Flatten(input_shape=input_shape))
+    #First layer
+    model.add(Dense(layer_size, activation='relu'))
+    model.add(Dropout(dropout_amount))
+
+    #Second layer
+    model.add(Dense(layer_size, activation='relu'))
+    model.add(Dropout(dropout_amount))
+
+    #Third layer
+    model.add(Dense(layer_size, activation='relu'))
+    model.add(Dropout(dropout_amount))
+
+    #Output Layer
+    model.add(Dense(num_classes, activation='softmax'))
+
     ##### Your code above (Lab 1)
 
     return model
diff --git a/lab1/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5 b/lab1/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5
new file mode 100644
index 0000000..97f9cf4
Binary files /dev/null and b/lab1/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5 differ
diff --git a/lab2/text_recognizer/networks/lenet.py b/lab2/text_recognizer/networks/lenet.py
index 19b06cd..cdc23ea 100644
--- a/lab2/text_recognizer/networks/lenet.py
+++ b/lab2/text_recognizer/networks/lenet.py
@@ -9,7 +9,34 @@ def lenet(input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> Model:
     num_classes = output_shape[0]
 
     ##### Your code below (Lab 2)
+    model = Sequential()
+    if len(input_shape) < 3:
+        model.add(Lambda(lambda x: tf.expand_dims(x, -1), input_shape=input_shape))
+        input_shape = (input_shape[0], input_shape[1], 1)
 
+    model.add(Conv2D(32, 
+                     kernel_size = (3,3), 
+                     strides = (1,1),
+                    activation = 'relu',
+                    input_shape = input_shape))
+    model.add(MaxPooling2D(pool_size = (2,2),
+                          strides = (2,2)))
+    
+    model.add(Conv2D(64, (5, 5), activation='relu'))
+
+    model.add(MaxPooling2D(pool_size=(2, 2)))
+
+    model.add(Flatten())
+    model.add(Dense(1000, activation='relu'))
+    model.add(Dropout(0.4))
+
+    model.add(Dense(1000, activation='relu'))
+    model.add(Dropout(0.4))
+    
+    model.add(Dense(1000, activation='relu'))
+    model.add(Dropout(0.4))
+    
+    model.add(Dense(num_classes, activation='softmax'))
     ##### Your code above (Lab 2)
 
     return model
diff --git a/lab2/text_recognizer/networks/line_cnn_sliding_window.py b/lab2/text_recognizer/networks/line_cnn_sliding_window.py
index aa28c3b..d973017 100644
--- a/lab2/text_recognizer/networks/line_cnn_sliding_window.py
+++ b/lab2/text_recognizer/networks/line_cnn_sliding_window.py
@@ -42,7 +42,16 @@ def line_cnn_sliding_window(
     convnet = KerasModel(inputs=convnet.inputs, outputs=convnet.layers[-2].output)
 
     convnet_outputs = TimeDistributed(convnet)(image_patches)
-    # (num_windows, 128)
+    # (num_windows, 1000)
+    convnet_outputs_extra_dim = Lambda(lambda x: tf.expand_dims(x, -1))(convnet_outputs)
+      # (num_windows, 1000, 1)
+    num_windows = int((image_width - window_width) / window_stride) + 1
+    width = int(num_windows / output_length)
+    
+    conved_convnet_outputs = Conv2D(num_classes, (width, 1000), (width, 1), activation='softmax')(convnet_outputs_extra_dim)
+
+    squeezed_conved_convnet_outputs = Lambda(lambda x: tf.squeeze(x, 2))(conved_convnet_outputs)
+    softmax_output = Lambda(lambda x: x[:, :output_length, :])(squeezed_conved_convnet_outputs)
 
     # Now we have to get to (output_length, num_classes) shape. One way to do it is to do another sliding window with
     # width = floor(num_windows / output_length)
diff --git a/lab3/tasks/train_lstm_line_predictor.sh b/lab3/tasks/train_lstm_line_predictor.sh
index 7afd0ba..de95f6c 100755
--- a/lab3/tasks/train_lstm_line_predictor.sh
+++ b/lab3/tasks/train_lstm_line_predictor.sh
@@ -1,2 +1,7 @@
 #!/bin/sh
-pipenv run python training/run_experiment.py --save '{"dataset": "EmnistLinesDataset", "model": "LineModelCtc", "network": "line_lstm_ctc"}'
+pipenv run python training/run_experiment.py --save '{"dataset": "EmnistLinesDataset", "model": "LineModelCtc", "network": "line_lstm_ctc",
+"train_args": {
+            "batch_size": 64,
+            "epochs": 16
+        }
+        }' --gpu $1
diff --git a/lab3/text_recognizer/models/line_model_ctc.py b/lab3/text_recognizer/models/line_model_ctc.py
index e4620a5..2b1f98a 100644
--- a/lab3/text_recognizer/models/line_model_ctc.py
+++ b/lab3/text_recognizer/models/line_model_ctc.py
@@ -80,7 +80,17 @@ class LineModelCtc(Model):
 
         # Get the prediction and confidence using softmax_output_fn, passing the right input into it.
         ##### Your code below (Lab 3)
+        input_image = np.expand_dims(image, 0)
+        softmax_output = softmax_output_fn([input_image, 0])[0]
 
+        input_length = np.array([softmax_output.shape[1]])
+        decoded, log_prob = K.ctc_decode(softmax_output, input_length, greedy=True)
+
+        pred_raw = K.eval(decoded[0])[0]
+        pred = ''.join(self.data.mapping[label] for label in pred_raw).strip()
+
+        neg_sum_logit = K.eval(log_prob)[0][0]
+        conf = np.exp(neg_sum_logit) / (1 + np.exp(neg_sum_logit))
         ##### Your code above (Lab 3)
 
         return pred, conf
diff --git a/lab3/text_recognizer/networks/lenet.py b/lab3/text_recognizer/networks/lenet.py
index 3cd0f1a..e95cc8c 100644
--- a/lab3/text_recognizer/networks/lenet.py
+++ b/lab3/text_recognizer/networks/lenet.py
@@ -13,13 +13,30 @@ def lenet(input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> Model:
     if len(input_shape) < 3:
         model.add(Lambda(lambda x: tf.expand_dims(x, -1), input_shape=input_shape))
         input_shape = (input_shape[0], input_shape[1], 1)
-    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
-    model.add(Conv2D(64, (3, 3), activation='relu'))
+    model.add(Conv2D(16, 
+                     kernel_size=(3, 3), 
+                     activation='relu', 
+                     input_shape=input_shape))
+
+    model.add(MaxPooling2D(pool_size = (2,2),
+                      strides = (2,2)))
+    
+    model.add(Conv2D(32, 
+                     (3, 3), 
+                     activation='relu'))
     model.add(MaxPooling2D(pool_size=(2, 2)))
-    model.add(Dropout(0.2))
+    model.add(Conv2D(64, 
+                     (2, 2), 
+                     activation='relu'))
+    model.add(MaxPooling2D(pool_size=(1, 1)))
     model.add(Flatten())
-    model.add(Dense(128, activation='relu'))
+    model.add(Dense(1200, activation='relu'))
+    model.add(Dropout(0.25))
+    model.add(Dense(800, activation='relu'))
+    model.add(Dropout(0.25))
+    model.add(Dense(400, activation='relu'))
     model.add(Dropout(0.2))
+    
     model.add(Dense(num_classes, activation='softmax'))
     ##### Your code above (Lab 2)
 
diff --git a/lab3/text_recognizer/networks/line_lstm_ctc.py b/lab3/text_recognizer/networks/line_lstm_ctc.py
index 25c1798..3ce928b 100644
--- a/lab3/text_recognizer/networks/line_lstm_ctc.py
+++ b/lab3/text_recognizer/networks/line_lstm_ctc.py
@@ -12,11 +12,16 @@ from text_recognizer.networks.misc import slide_window
 from text_recognizer.networks.ctc import ctc_decode
 
 
-def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
+def line_lstm_ctc(input_shape, 
+                  output_shape, 
+                  window_width=28, 
+                  window_stride=14):
+    
     image_height, image_width = input_shape
     output_length, num_classes = output_shape
 
     num_windows = int((image_width - window_width) / window_stride) + 1
+    
     if num_windows < output_length:
         raise ValueError(f'Window width/stride need to generate at least {output_length} windows (currently {num_windows})')
 
@@ -35,6 +40,22 @@ def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
     # Note that lstms expect a input of shape (num_batch_size, num_timesteps, feature_length).
 
     ##### Your code below (Lab 3)
+    image_reshaped = Reshape((image_height, image_width, 1))(image_input)
+    # (image_height, image_width, 1)
+    
+    image_patches = Lambda(
+        slide_window,
+        arguments={'window_width': window_width, 'window_stride': window_stride}
+    )(image_reshaped)
+    
+    convnet = lenet((image_height, window_width, 1), (num_classes,))
+    convnet = KerasModel(inputs=convnet.inputs, outputs=convnet.layers[-2].output)
+
+    convnet_outputs = TimeDistributed(convnet)(image_patches)
+    # (num_windows, 400)
+    lstm_output = lstm_fn(400, return_sequences=True)(convnet_outputs)
+    
+    softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_output)    
 
     ##### Your code above (Lab 3)
 
diff --git a/lab3/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5 b/lab3/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5
new file mode 100644
index 0000000..15d60be
Binary files /dev/null and b/lab3/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5 differ
diff --git a/lab4/tasks/train_lstm_line_predictor.sh b/lab4/tasks/train_lstm_line_predictor.sh
index 7afd0ba..1f206f7 100755
--- a/lab4/tasks/train_lstm_line_predictor.sh
+++ b/lab4/tasks/train_lstm_line_predictor.sh
@@ -1,2 +1,8 @@
 #!/bin/sh
-pipenv run python training/run_experiment.py --save '{"dataset": "EmnistLinesDataset", "model": "LineModelCtc", "network": "line_lstm_ctc"}'
+pipenv run python training/run_experiment.py --save '{"dataset": "EmnistLinesDataset", "model": "LineModelCtc", "network": "line_lstm_ctc",
+"train_args": {
+            "batch_size": 32,
+            "epochs": 16
+        }
+        }' --gpu $1
+
diff --git a/lab4/text_recognizer/networks/lenet.py b/lab4/text_recognizer/networks/lenet.py
index 3cd0f1a..f6b2888 100644
--- a/lab4/text_recognizer/networks/lenet.py
+++ b/lab4/text_recognizer/networks/lenet.py
@@ -13,15 +13,33 @@ def lenet(input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> Model:
     if len(input_shape) < 3:
         model.add(Lambda(lambda x: tf.expand_dims(x, -1), input_shape=input_shape))
         input_shape = (input_shape[0], input_shape[1], 1)
-    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
-    model.add(Conv2D(64, (3, 3), activation='relu'))
+    model.add(Conv2D(32, 
+                     kernel_size=(3, 3), 
+                     activation='relu', 
+                     input_shape=input_shape))
+
+    model.add(MaxPooling2D(pool_size = (2,2),
+                      strides = (2,2)))
+    
+    model.add(Conv2D(64, 
+                     (3, 3), 
+                     activation='relu'))
     model.add(MaxPooling2D(pool_size=(2, 2)))
-    model.add(Dropout(0.2))
+    model.add(Conv2D(128, 
+                     (2, 2), 
+                     activation='relu'))
+    
+    model.add(MaxPooling2D(pool_size=(1, 1)))
+    
     model.add(Flatten())
-    model.add(Dense(128, activation='relu'))
-    model.add(Dropout(0.2))
+    model.add(Dense(600, activation='relu'))
+    model.add(Dropout(0.3))
+    model.add(Dense(200, activation='relu'))
+    model.add(Dropout(0.3))
+    
     model.add(Dense(num_classes, activation='softmax'))
     ##### Your code above (Lab 2)
 
     return model
 
+
diff --git a/lab4/text_recognizer/networks/line_lstm_ctc.py b/lab4/text_recognizer/networks/line_lstm_ctc.py
index 90d7325..4fc32d4 100644
--- a/lab4/text_recognizer/networks/line_lstm_ctc.py
+++ b/lab4/text_recognizer/networks/line_lstm_ctc.py
@@ -12,11 +12,16 @@ from text_recognizer.networks.misc import slide_window
 from text_recognizer.networks.ctc import ctc_decode
 
 
-def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
+def line_lstm_ctc(input_shape, 
+                  output_shape, 
+                  window_width=28, 
+                  window_stride=14):
+    
     image_height, image_width = input_shape
     output_length, num_classes = output_shape
 
     num_windows = int((image_width - window_width) / window_stride) + 1
+    
     if num_windows < output_length:
         raise ValueError(f'Window width/stride need to generate at least {output_length} windows (currently {num_windows})')
 
@@ -37,24 +42,21 @@ def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
     ##### Your code below (Lab 3)
     image_reshaped = Reshape((image_height, image_width, 1))(image_input)
     # (image_height, image_width, 1)
-
+    
     image_patches = Lambda(
         slide_window,
         arguments={'window_width': window_width, 'window_stride': window_stride}
     )(image_reshaped)
-    # (num_windows, image_height, window_width, 1)
-
-    # Make a LeNet and get rid of the last two layers (softmax and dropout)
+    
     convnet = lenet((image_height, window_width, 1), (num_classes,))
     convnet = KerasModel(inputs=convnet.inputs, outputs=convnet.layers[-2].output)
-    convnet_outputs = TimeDistributed(convnet)(image_patches)
-    # (num_windows, 128)
 
-    lstm_output = lstm_fn(128, return_sequences=True)(convnet_outputs)
-    # (num_windows, 128)
+    convnet_outputs = TimeDistributed(convnet)(image_patches)
+    # (num_windows, 200)
+    lstm_output = lstm_fn(200, return_sequences=True)(convnet_outputs)
+    
+    softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_output)    
 
-    softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_output)
-    # (num_windows, num_classes)
     ##### Your code above (Lab 3)
 
     input_length_processed = Lambda(
@@ -78,3 +80,4 @@ def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
     )
     return model
 
+
diff --git a/lab4/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5 b/lab4/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5
index 86a801d..076237a 100644
Binary files a/lab4/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5 and b/lab4/text_recognizer/weights/CharacterModel_EmnistDataset_mlp_weights.h5 differ
diff --git a/lab4/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5 b/lab4/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5
index 423a59f..3f9c14c 100644
Binary files a/lab4/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5 and b/lab4/text_recognizer/weights/LineModelCtc_EmnistLinesDataset_line_lstm_ctc_weights.h5 differ
diff --git a/lab5/tasks/train_lstm_line_predictor_on_iam.sh b/lab5/tasks/train_lstm_line_predictor_on_iam.sh
index fbc61f5..50d4ea4 100755
--- a/lab5/tasks/train_lstm_line_predictor_on_iam.sh
+++ b/lab5/tasks/train_lstm_line_predictor_on_iam.sh
@@ -1,2 +1,14 @@
 #!/bin/sh
-pipenv run python training/run_experiment.py --save '{"dataset": "IamLinesDataset", "model": "LineModelCtc", "network": "line_lstm_ctc"}'
+pipenv run python training/run_experiment.py --save --gpu -1 '{"dataset": "IamLinesDataset", 
+"model": "LineModelCtc", 
+"network": "line_lstm_ctc",
+"network_args": {
+"window_width": 6,
+"window_stride": 2
+},
+"train_args": {
+            "batch_size": 32,
+            "epochs": 64,
+            "user_name": "hoktay"
+        }
+        }'
\ No newline at end of file
diff --git a/lab5/text_recognizer/networks/lenet.py b/lab5/text_recognizer/networks/lenet.py
index 3cd0f1a..6818dd9 100644
--- a/lab5/text_recognizer/networks/lenet.py
+++ b/lab5/text_recognizer/networks/lenet.py
@@ -24,4 +24,3 @@ def lenet(input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> Model:
     ##### Your code above (Lab 2)
 
     return model
-
diff --git a/lab5/text_recognizer/networks/line_lstm_ctc.py b/lab5/text_recognizer/networks/line_lstm_ctc.py
index 90d7325..652ba1f 100644
--- a/lab5/text_recognizer/networks/line_lstm_ctc.py
+++ b/lab5/text_recognizer/networks/line_lstm_ctc.py
@@ -12,7 +12,7 @@ from text_recognizer.networks.misc import slide_window
 from text_recognizer.networks.ctc import ctc_decode
 
 
-def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
+def line_lstm_ctc(input_shape, output_shape, window_width=6, window_stride=2):
     image_height, image_width = input_shape
     output_length, num_classes = output_shape
 
@@ -50,10 +50,16 @@ def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
     convnet_outputs = TimeDistributed(convnet)(image_patches)
     # (num_windows, 128)
 
-    lstm_output = lstm_fn(128, return_sequences=True)(convnet_outputs)
+    lstm_output = Bidirectional(lstm_fn(128, return_sequences=True))(convnet_outputs)
     # (num_windows, 128)
 
-    softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_output)
+    lstm_output_1_drop_out = Dropout(0.2)(lstm_output)
+    
+    lstm_output2 = Bidirectional(lstm_fn(128, return_sequences=True))(lstm_output_1_drop_out)
+
+    lstm_output_2_drop_out = Dropout(0.2)(lstm_output2)
+
+    softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_output_2_drop_out)
     # (num_windows, num_classes)
     ##### Your code above (Lab 3)
 
@@ -76,5 +82,4 @@ def line_lstm_ctc(input_shape, output_shape, window_width=28, window_stride=14):
         inputs=[image_input, y_true, input_length, label_length],
         outputs=[ctc_loss_output, ctc_decoded_output]
     )
-    return model
-
+    return model
\ No newline at end of file
diff --git a/lab5/training/run_experiment.py b/lab5/training/run_experiment.py
index eb35525..aed45de 100755
--- a/lab5/training/run_experiment.py
+++ b/lab5/training/run_experiment.py
@@ -14,8 +14,8 @@ from training.util import train_model
 
 
 DEFAULT_TRAIN_ARGS = {
-    'batch_size': 64,
-    'epochs': 8
+    'batch_size': 32,
+    'epochs': 4
 }
 
 
